{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c47fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from moviepy import ImageSequenceClip\n",
    "import os \n",
    "\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "TARGET_SIZE=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5659203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset_weight_estimation\\\\dataset_weight_estimation\"\n",
    "vid_carry = cv2.VideoCapture(dataset_path + \"\\\\empty\\\\ope1\\\\carry\\\\angle3.mp4\")\n",
    "vid_walk  = cv2.VideoCapture(dataset_path + \"\\\\empty\\\\ope1\\\\walk\\\\angle1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7ed9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path, target_size=TARGET_SIZE, target_frames=1000):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frames = []\n",
    "\n",
    "    # Read frames and resize\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, target_size)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "\n",
    "    # Pad if needed\n",
    "    if len(frames) < target_frames:\n",
    "        last_frame = frames[-1]\n",
    "        frames.extend([last_frame] * (target_frames - len(frames)))\n",
    "\n",
    "    # Trim if video is longer\n",
    "    frames = frames[:target_frames]\n",
    "\n",
    "    # Save as video\n",
    "    clip = ImageSequenceClip([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames], fps=25)\n",
    "    clip.write_videofile(output_path, codec=\"libx264\")\n",
    "\n",
    "    #print(f\"Saved processed video: {output_path}\")\n",
    "\n",
    "\n",
    "# Example\n",
    "#process_video(\"dataset_weight_estimation\\\\dataset_weight_estimation\\\\empty\\\\ope1\\\\carry\\\\angle3.mp4\",\"processed_empty_224_1000.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fdc875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video processed_heavy_224_1000.mp4.\n",
      "MoviePy - Writing video processed_heavy_224_1000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready processed_heavy_224_1000.mp4\n",
      "Saved processed video: processed_heavy_224_1000.mp4\n"
     ]
    }
   ],
   "source": [
    "process_video(\"dataset_weight_estimation\\\\dataset_weight_estimation\\\\heavy\\\\ope1\\\\carry\\\\angle3.mp4\",\"processed_heavy_224_1000.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2825e",
   "metadata": {},
   "source": [
    "## Preprocessing dataset\n",
    "### shape (224,224) , frames 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8fcc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"dataset_weight_estimation\\\\dataset_weight_estimation\\\\\"\n",
    "OUT_PATH = \"Preprocessed_data\"\n",
    "os.makedirs(OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc3a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"empty\",\"heavy\",\"light\"]:\n",
    "    for i in range(8) : \n",
    "        for j in range(3) :\n",
    "            vid_path = DATA_DIR + f\"\\\\{label}\\\\ope{i+1}\\\\carry\\\\angle{j+1}.mp4\"\n",
    "            out_path = f\"\\\\{label}\\\\ope{i+1}\\\\carry\"\n",
    "            if not os.path.exists(OUT_PATH + out_path) : \n",
    "                os.makedirs(OUT_PATH+out_path)\n",
    "            \n",
    "            process_video(vid_path,OUT_PATH + out_path + f\"\\\\angle{j+1}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f296392",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"empty\",\"heavy\",\"light\"]:\n",
    "    for i in range(8) : \n",
    "        for j in range(3) :\n",
    "            vid_path = DATA_DIR + f\"\\\\{label}\\\\ope{i+1}\\\\walk\\\\angle{j+1}.mp4\"\n",
    "            out_path = f\"\\\\{label}\\\\ope{i+1}\\\\walk\"\n",
    "            if not os.path.exists(OUT_PATH + out_path) : \n",
    "                os.makedirs(OUT_PATH+out_path)\n",
    "            process_video(vid_path,OUT_PATH + out_path + f\"\\\\angle{j+1}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41cddcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### KEYPOINTS EXTRACTION\n",
    "import torch\n",
    "from random import random\n",
    "import torchvision \n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv11-pose model...\n",
      "YOLO model loaded successfully on device: cuda\n",
      "Processing video: processed_empty_224_1000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing with YOLO: 100%|██████████| 1000/1000 [00:16<00:00, 61.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization complete! Video saved to: keypoint_visualization_yolo.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "VID = \"processed_empty_224_1000.mp4\" \n",
    "OUTPUT_VIDEO_PATH = \"keypoint_visualization_yolo.mp4\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- Load YOLOv11-Pose Model ---\n",
    "try:\n",
    "    print(\"Loading YOLOv11-pose model...\")\n",
    "    # Using 'yolov8n-pose.pt' as it's a standard small model. \n",
    "    # Replace with 'yolo11n-pose.pt' if you have that specific file.\n",
    "    model = YOLO(\"yolo11n-pose.pt\") \n",
    "    model.to(DEVICE)\n",
    "    print(f\"YOLO model loaded successfully on device: {DEVICE}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading YOLO model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Keypoint Drawing Utilities (No changes needed here) ---\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
    "    'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
    "    'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
    "    'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
    "}\n",
    "SKELETON_EDGES = [\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['right_shoulder']), (KEYPOINT_DICT['left_hip'], KEYPOINT_DICT['right_hip']),\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['left_hip']), (KEYPOINT_DICT['right_shoulder'], KEYPOINT_DICT['right_hip']),\n",
    "    (KEYPOINT_DICT['left_shoulder'], KEYPOINT_DICT['left_elbow']), (KEYPOINT_DICT['left_elbow'], KEYPOINT_DICT['left_wrist']),\n",
    "    (KEYPOINT_DICT['right_shoulder'], KEYPOINT_DICT['right_elbow']), (KEYPOINT_DICT['right_elbow'], KEYPOINT_DICT['right_wrist']),\n",
    "    (KEYPOINT_DICT['left_hip'], KEYPOINT_DICT['left_knee']), (KEYPOINT_DICT['left_knee'], KEYPOINT_DICT['left_ankle']),\n",
    "    (KEYPOINT_DICT['right_hip'], KEYPOINT_DICT['right_knee']), (KEYPOINT_DICT['right_knee'], KEYPOINT_DICT['right_ankle']),\n",
    "    (KEYPOINT_DICT['nose'], KEYPOINT_DICT['left_eye']), (KEYPOINT_DICT['nose'], KEYPOINT_DICT['right_eye']),\n",
    "    (KEYPOINT_DICT['left_eye'], KEYPOINT_DICT['left_ear']), (KEYPOINT_DICT['right_eye'], KEYPOINT_DICT['right_ear']),\n",
    "]\n",
    "\n",
    "def draw_keypoints(frame, keypoints, confidence_threshold=0.2):\n",
    "    h, w, _ = frame.shape\n",
    "    for kp in keypoints:\n",
    "        y, x, conf = kp\n",
    "        if conf > confidence_threshold:\n",
    "            # Note: keypoints are already normalized [y, x], so we multiply by h, w\n",
    "            cv2.circle(frame, (int(x * w), int(y * h)), 4, (0, 255, 0), -1)\n",
    "\n",
    "def draw_skeleton(frame, keypoints, confidence_threshold=0.2):\n",
    "    h, w, _ = frame.shape\n",
    "    for start_idx, end_idx in SKELETON_EDGES:\n",
    "        start_kp, end_kp = keypoints[start_idx], keypoints[end_idx]\n",
    "        if start_kp[2] > confidence_threshold and end_kp[2] > confidence_threshold:\n",
    "            start_point = (int(start_kp[1] * w), int(start_kp[0] * h))\n",
    "            end_point = (int(end_kp[1] * w), int(end_kp[0] * h))\n",
    "            cv2.line(frame, start_point, end_point, (255, 0, 0), 2)\n",
    "\n",
    "def get_keypoints_from_result(result, frame_h, frame_w):\n",
    "    \"\"\"\n",
    "    Extracts, normalizes, and formats keypoints from a YOLO result.\n",
    "    Handles multi-person detection by picking the most confident person.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array of shape (17, 3) with normalized [y, x, confidence].\n",
    "    \"\"\"\n",
    "    if result.keypoints is None or len(result.keypoints.data) == 0:\n",
    "        return np.zeros((17, 3), dtype=np.float32)\n",
    "\n",
    "    kpts_tensor = result.keypoints.data\n",
    "    \n",
    "    # --- Handle multiple people: select the one with the highest avg confidence ---\n",
    "    if kpts_tensor.shape[0] > 1:\n",
    "        confidences = kpts_tensor[:, :, 2].mean(dim=1)\n",
    "        best_person_idx = confidences.argmax()\n",
    "        person_kpts = kpts_tensor[best_person_idx]\n",
    "    else:\n",
    "        person_kpts = kpts_tensor[0]\n",
    "\n",
    "    # Convert to numpy and normalize\n",
    "    person_kpts_np = person_kpts.cpu().numpy()\n",
    "    \n",
    "    # Format to [y, x, confidence] and normalize\n",
    "    formatted_kps = np.zeros((17, 3), dtype=np.float32)\n",
    "    for i in range(17):\n",
    "        x, y, conf = person_kpts_np[i]\n",
    "        formatted_kps[i] = [y / frame_h, x / frame_w, conf]\n",
    "        \n",
    "    return formatted_kps\n",
    "\n",
    "def process_and_visualize_video(video_path: str, output_path: str, yolo_model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width, frame_height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps, num_frames = int(cap.get(cv2.CAP_PROP_FPS)), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
    "    for _ in tqdm(range(num_frames), desc=\"Visualizing with YOLO\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        # --- Run YOLO Inference ---\n",
    "        # The model expects BGR frames, which cv2.read() provides\n",
    "        results = yolo_model(frame, verbose=False)\n",
    "        \n",
    "        # --- Extract, normalize, and format keypoints ---\n",
    "        # We process the first (and likely only) result\n",
    "        keypoints = get_keypoints_from_result(results[0], frame_height, frame_width)\n",
    "\n",
    "        # --- Drawing on the frame (this happens on CPU) ---\n",
    "        vis_frame = frame.copy()\n",
    "        draw_keypoints(vis_frame, keypoints)\n",
    "        draw_skeleton(vis_frame, keypoints)\n",
    "        out.write(vis_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nVisualization complete! Video saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    process_and_visualize_video(VID, OUTPUT_VIDEO_PATH, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcb56cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data for video: processed_heavy_224_1000.mp4\n"
     ]
    }
   ],
   "source": [
    "### Background subtraction\n",
    "VID = \"processed_heavy_224_1000.mp4\"\n",
    "VID_NAME  = \"processed_heavy_224_1000\" \n",
    "cap = cv2.VideoCapture(VID)\n",
    "ret,prev_frame = cap.read()\n",
    "\n",
    "if not ret: \n",
    "    print(f\"[ERROR] Cannot read first frame\")\n",
    "h,w = prev_frame.shape[:2]\n",
    "\n",
    "bg = cv2.createBackgroundSubtractorMOG2(history=400,varThreshold=40,detectShadows=False)\n",
    "prev_gray = cv2.cvtColor(prev_frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "fg_masks = []\n",
    "flows = []\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out_mask = cv2.VideoWriter(\n",
    "    f\"{VID_NAME}_fg_mask.mp4\",\n",
    "    fourcc,\n",
    "    cap.get(cv2.CAP_PROP_FPS),\n",
    "    (w, h),\n",
    "    isColor=False\n",
    ")\n",
    "while True : \n",
    "    ret,frame = cap.read()\n",
    "    if not ret : \n",
    "        break\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # ===== 1. Background Subtraction =====\n",
    "    fg_mask = bg.apply(frame)\n",
    "    _,fg_mask = cv2.threshold(fg_mask,250,255,cv2.THRESH_BINARY)\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)))\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))) \n",
    "    out_mask.write(fg_mask)\n",
    "    # ===== 2. Optical Flow =====\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray,\n",
    "                                        None,\n",
    "                                        0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Save for deep learning\n",
    "    fg_masks.append(fg_mask.astype(np.uint8))         # (H, W)\n",
    "    flows.append(flow.astype(np.float32))            # (H, W, 2)\n",
    "\n",
    "    prev_gray = gray\n",
    "out_mask.release()\n",
    "cap.release()\n",
    "video_name = os.path.splitext(VID)[0]\n",
    "np.save(os.path.join(\".\",f\"{video_name}_fg.npy\"), np.array(fg_masks))\n",
    "np.save(os.path.join(\".\" , f\"{video_name}_flow.npy\"), np.array(flows))\n",
    "\n",
    "print(\"Saved processed data for video:\" ,VID)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
